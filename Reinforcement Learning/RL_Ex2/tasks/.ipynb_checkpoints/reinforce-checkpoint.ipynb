{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part A\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2021-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question A2 - Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: The next cell is optional, as this will _not_ run on the JupterHub. To render the CartPole environemnt, you need to set up your Jupyter environment locally (see assignment PDF). Rendering is not required for this assignment, but the visualization may help you to understand what your policy is atually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the environment (Does not run on the JupyterHub)\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Defining the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # ********************\n",
    "        # TODO Create layers\n",
    "        self.mlp1 = nn.Linear(4,128)\n",
    "        self.mlp2 = nn.Linear(128,2)\n",
    "        # ********************\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ********************\n",
    "        # TODO Implement forward pass\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = self.mlp2(x)\n",
    "        \n",
    "        x = F.softmax(x,dim=1)\n",
    "        # ********************\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_output = Policy()(torch.tensor([[1.0, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "np.testing.assert_almost_equal(_test_output.detach().numpy().sum(), 1, err_msg=\"Output is not a probability distribution.\")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_action(probs):\n",
    "    \"\"\"Sample one action from the action distribution of this state.\n",
    "    \n",
    "    Args:\n",
    "        probs: action probabilities\n",
    "\n",
    "    Returns:\n",
    "        action: The sampled action\n",
    "        log_prob: Logarithm of the probability for sampling that action\n",
    "    \"\"\"\n",
    "    # TODO Implement action sampling    \n",
    "    probsTensor = torch.distributions.Categorical(probs)\n",
    "    action = probsTensor.sample()\n",
    "    log_prob = probsTensor.log_prob(action)\n",
    "    \n",
    "    return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_action, _test_log_prob = sample_action(torch.tensor([1, 2, 3, 4]))\n",
    "assert _test_action in [0, 1, 2, 3], f\"Invalid action {_test_action}\"\n",
    "np.testing.assert_approx_equal(_test_log_prob, np.log((_test_action + 1) / 10))\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Return Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_return(rewards, gamma=0.99):\n",
    "    \"\"\"Estimate return based of observed rewards\n",
    "    \n",
    "    Args:\n",
    "        rewards: Series of observed rewards\n",
    "        gamma: discount factor\n",
    "    \"\"\"\n",
    "    # TODO Implement return estimation\n",
    "    returns = np.zeros_like(rewards)\n",
    "    for i in range(rewards.size):\n",
    "        for j in range(i,rewards.size):\n",
    "            returns[i] += gamma**(j-i)*rewards[j]\n",
    "    returns = (returns - np.mean(returns)) / np.std(returns)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "np.testing.assert_array_almost_equal(\n",
    "    estimate_return(np.ones(10), gamma=0.99),\n",
    "    [1.54572815, 1.21139962, 0.87369404, 0.53257729, 0.18801491, -0.16002789, -0.51158628, -0.86669576, -1.22539221, -1.58771185]\n",
    ")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3f98a3d6964135a9e3da0d96173a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03773089  0.03920658 -0.01686263  0.01619539]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sample_action() missing 1 required positional argument: 'probs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [138]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Take a step\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# ********************\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# TODO Sample action for current state\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[0;32m---> 22\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#torch.tensor(state))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action, log_prob)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# ********************\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_action() missing 1 required positional argument: 'probs'"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "\n",
    "# Hyperparams\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "learn_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learn_rate)\n",
    "\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        # Run one episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            # ********************\n",
    "            # TODO Sample action for current state\n",
    "            print(state)\n",
    "            action, log_prob = sample_action()  #torch.tensor(state))\n",
    "            \n",
    "            print(action, log_prob)\n",
    "            # ********************\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # ********************\n",
    "        # TODO Compute loss\n",
    "\n",
    "        policy_loss = -(log_probs*rewards)\n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2d.pt\", \"wb\") as f:\n",
    "    torch.save(policy, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c9bd3345741efb306253e953c84c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Average reward below 487.5, got 19.79",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\u001b[38;5;241m.\u001b[39m_elapsed_steps\n\u001b[1;32m     15\u001b[0m _avg_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([_rollout(seed\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _avg_reward \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m487.5\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage reward below 487.5, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_avg_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok (Average reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_avg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Average reward below 487.5, got 19.79"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.seed(seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the learned policy (this will not run on the JupyterHub)\n",
    "greedy = True\n",
    "\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "    if greedy:\n",
    "        action = np.argmax(probs.detach().numpy())  # Chose optimal action\n",
    "    else:\n",
    "        action = sample_action(probs)[0]  # Sample from action distribution\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
