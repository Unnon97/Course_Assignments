{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part B\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2022-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question B1 - Deep Q-Networks\n",
    "### a) Implement Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, mem_size, state_shape):\n",
    "        \"\"\"Initialization of the replay buffer.\n",
    "        \n",
    "        The memories have the following data types:\n",
    "            states: float32\n",
    "            next_states: float32\n",
    "            actions: int64\n",
    "            rewards: float32\n",
    "            is_terminal: bool\n",
    "\n",
    "        Args:\n",
    "            mem_size: Capacity of this buffer\n",
    "            state_shape: Shape of state and next_state\n",
    "        \"\"\"\n",
    "        self.mem_size = mem_size  # Capacity of the buffer\n",
    "        self.mem_cntr = 0         # Number of added elements\n",
    "        self.state_memory = np.zeros((self.mem_size, *state_shape), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((self.mem_size, *state_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "    \n",
    "    def is_filled(self):\n",
    "        \"\"\"Check if the memory is filled.\"\"\"\n",
    "        return buffer.mem_cntr >= buffer.mem_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, is_terminal):\n",
    "        \"\"\"Add one transition to the buffer.\n",
    "\n",
    "        Replaces the oldest transition in memory.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        if state in self.state_memory:\n",
    "            indexx = np.where(state in self.state_memory)\n",
    "        elif state not in self.state_memory:\n",
    "            indexx = np.min(np.where(self.state_memory.any(axis=1)==0)[0])\n",
    "            \n",
    "        self.state_memory[indexx] = state\n",
    "        self.next_state_memory[indexx] = next_state\n",
    "        self.action_memory[indexx] = action\n",
    "        self.reward_memory[indexx] = reward\n",
    "        self.terminal_memory[indexx] = is_terminal\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"Sample one batch from the memory.\"\"\"\n",
    "        # TODO\n",
    "        idx = np.random.choice(self.state_memory[:,0],batch_size,replace=False).astype(int)\n",
    "        states = self.state_memory[idx]\n",
    "        actions = self.action_memory[idx]\n",
    "        rewards = self.reward_memory[idx]\n",
    "        is_terminal = self.terminal_memory[idx]\n",
    "        next_states = self.next_state_memory[idx]\n",
    "        return states, actions, rewards, next_states, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_buffer = ReplayBuffer(10, (5, ))\n",
    "assert _buffer.mem_size == 10\n",
    "assert _buffer.mem_cntr == 0\n",
    "for i in range(10):  # Fill test values\n",
    "    _buffer.add(np.arange(5) + i, 5 + i, 6 + i, 7 + np.arange(5) + i, (12 + i) % 2 == 0)\n",
    "assert _buffer.mem_size == 10\n",
    "assert _buffer.mem_cntr == 10, \"Wrong mem_cntr\"\n",
    "\n",
    "_is = set()\n",
    "for s, a, r, s_, t in zip(*_buffer.sample_batch(5)):\n",
    "    i = s[0]\n",
    "    assert 0 <= i < 10, \"Wrong states\"\n",
    "    _is.add(i)\n",
    "    np.testing.assert_array_equal(s, np.arange(5) + i, err_msg=\"Wrong states\")\n",
    "    np.testing.assert_equal(a, 5 + i, err_msg=\"Wrong actions\")\n",
    "    np.testing.assert_equal(r, 6 + i, err_msg=\"Wrong rewards\")\n",
    "    np.testing.assert_array_equal(s_, 7 + np.arange(5) + i, err_msg=\"Wrong next states\")\n",
    "    np.testing.assert_equal(t, (12 + i) % 2 == 0, err_msg=\"Wrong terminals\")\n",
    "assert len(_is) == 5, \"Duplicate transitions\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Fill replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Initialize replay buffer\n",
    "buffer = ReplayBuffer(mem_size=25000, state_shape=env.observation_space.shape)\n",
    "\n",
    "# ********************\n",
    "# TODO Sample transitions from environment and add to buffer\n",
    "\n",
    "while not buffer.is_filled():\n",
    "    state = env.reset()\n",
    "    actt = env.action_space.sample()\n",
    "    next_state, reward, terminal,_ = env.step(actt)\n",
    "    buffer.add(state,actt,reward,next_state,terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "assert buffer.is_filled(), f\"Buffer not filled, only {buffer.mem_cntr}/{buffer.mem_size} transitions in memory\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question B2 - Deep Q-Networks\n",
    "### a) Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        # TODO Create layers\n",
    "        self.mlp1 = nn.Linear(4,256)\n",
    "        self.mlp2 = nn.Linear(256,2)\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO Implement forward pass\n",
    "        x = torch.relu(self.mlp1(state))\n",
    "        x = self.mlp2(x)\n",
    "        Q = F.softmax(x,dim=-1)\n",
    "\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_output = DeepQNetwork()(torch.FloatTensor([[1, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) $\\epsilon$-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, q_network, epsilon=0.05):\n",
    "    \"\"\"Perform epsilon-greedy action sampling.\n",
    "\n",
    "    Args:\n",
    "        state: numpy ndarray, current state\n",
    "        q_network: torch module\n",
    "\n",
    "    Returns:\n",
    "        action: one action\n",
    "    \"\"\"\n",
    "    # TODO Epsilon-greedy action sampling\n",
    "    poss_action = q_network((torch.FloatTensor(state))).detach().numpy()-1\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return (np.random.choice(poss_action).astype(np.int64))\n",
    "    return( np.argmax(poss_action).astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Code\n",
    "class DummyModule(nn.Module):\n",
    "    def forward(self, state):\n",
    "        return torch.FloatTensor([1, 2])  # Constant output\n",
    "\n",
    "\n",
    "# Sample 1000 actions\n",
    "_actions = [epsilon_greedy(np.array([1, 2, 3, 4]), DummyModule(), epsilon=0.2) for _ in range(1000)]\n",
    "\n",
    "sns.histplot(_actions, discrete=True, stat=\"density\")\n",
    "plt.xticks([0, 1])\n",
    "plt.show()\n",
    "\n",
    "_zeros = 1000 - sum(_actions)\n",
    "# Note: This is a stochastic test. It produces a false error in 1% of the cases\n",
    "assert 75 < _zeros < 125, f\"Frequency of action 0 ({_zeros}) is outside the 99% confidence interval [76, 124]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "\n",
    "def compute_loss(q_network, target_network, states, actions, rewards, next_states, is_terminal, gamma=0.99):\n",
    "    # TODO Implement loss function\n",
    "    qvals = q_network(torch.FloatTensor(states)).detach().numpy()\n",
    "    best_q = np.max(q_network(torch.FloatTensor(next_states)).detach().numpy()) \n",
    "    b = (1-is_terminal.int()).detach().numpy()\n",
    "    b = b.reshape((b.size,1))    \n",
    "    targetqvals = np.multiply((target_network(torch.FloatTensor(next_states)).detach().numpy()),b) #@ (1-is_terminal.int().detach().numpy() )  \n",
    "    best_targetq = np.max(targetqvals)\n",
    "    expected_qvals = np.ones_like(targetqvals)*(reward + gamma*best_targetq)\n",
    "    qvals = torch.Tensor(qvals)\n",
    "    expected_qvals = torch.Tensor(expected_qvals)\n",
    "    loss = mse(qvals, expected_qvals)\n",
    "    loss.requires_grad=True\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da9882b06b74a26b0d97247a358d5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Bookkeeping\u001b[39;00m\n\u001b[1;32m     29\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_terminal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_elapsed_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     33\u001b[0m step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mReplayBuffer.add\u001b[0;34m(self, state, action, reward, next_state, is_terminal)\u001b[0m\n\u001b[1;32m     35\u001b[0m     indexx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory:\n\u001b[0;32m---> 37\u001b[0m     indexx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_memory[indexx] \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_state_memory[indexx] \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/Coding/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2916\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[1;32m   2801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2802\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2803\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2805\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Coding/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epsilon = 0.05  # For epsilon greedy action sampling\n",
    "batch_size = 64\n",
    "NETWORK_UPDATE_FREQUENCY = 4\n",
    "NETWORK_SYNC_FREQUENCY = 2000\n",
    "gamma = 0.99\n",
    "episodes = 10000\n",
    "replay_buffer_size = 0 #TODO\n",
    "\n",
    "\n",
    "q_network = DeepQNetwork()\n",
    "target_network = deepcopy(q_network)\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=1e-3)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "step_count = 0\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        state, done = env.reset(), False\n",
    "        rewards = []\n",
    "\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            action = epsilon_greedy(state, q_network, epsilon=epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            rewards.append(reward)\n",
    "            buffer.add(state=state, action=action, reward=reward, next_state=next_state, is_terminal=done and env._elapsed_steps < 500)\n",
    "            state = next_state\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "            # Update network every NETWORK_UPDATE_FREQUENCY steps\n",
    "            if step_count % NETWORK_UPDATE_FREQUENCY == 0:\n",
    "                # Sample batch of transitions\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = buffer.sample_batch(batch_size=batch_size)\n",
    "                state_batch = torch.FloatTensor(state_batch)\n",
    "                next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "                action_batch = torch.LongTensor(action_batch).reshape(-1, 1)\n",
    "                reward_batch = torch.FloatTensor(reward_batch).reshape(-1, 1)\n",
    "                terminal_batch = torch.BoolTensor(terminal_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = compute_loss(q_network, target_network, state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, gamma)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "           \n",
    "            # Sync networks every NETWORK_SYNC_FREQUENCY steps\n",
    "            if step_count % NETWORK_SYNC_FREQUENCY == 0:\n",
    "                # ********************\n",
    "                # TODO Synchronize networks\n",
    "                target_network = deepcopy(q_network)\n",
    "\n",
    "\n",
    "\n",
    "                # ********************\n",
    "\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "         # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "        if np.mean(total_rewards[-100:]) == 500:\n",
    "            break # Stop training\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/b2d.pt\", \"wb\") as f:\n",
    "    torch.save(q_network, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/b2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test code\n",
    "policy = q_network\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.reset(seed=seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
